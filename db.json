{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1466836754050},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1466836754050},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1466836754050},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1466836754050},{"_id":"themes/landscape/_config.yml","hash":"fb8c98a0f6ff9f962637f329c22699721854cd73","modified":1466836754050},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1466836754050},{"_id":"source/_posts/hello-world.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1466836646071},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1466836754050},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1466836754050},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1466836754050},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1466836754050},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1466836754050},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1466836754050},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1466836754050},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1466836754050},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1466836754050},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1466836754050},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1466836754050},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1466836754050},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1466836754050},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1466836754050},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"82a30f81c0e8ba4a8af17acd6cc99e93834e4d5e","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"51670f2d47297e42e9ebc9cd7d9e78f0c2963e31","modified":1466837913107},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"c21ca56f419d01a9f49c27b6be9f4a98402b2aa3","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1466836754050},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1466836754050},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1466836754050},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1466836754050},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1466836754050},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1466836754050},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1466836754050},{"_id":"themes/landscape/source/css/_variables.styl","hash":"5e37a6571caf87149af83ac1cc0cdef99f117350","modified":1466836754050},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1466836754054},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1466836754054},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1466836754050},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1466836754050},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1466836754050},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1466836754050},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1466836754050},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1466836754050},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1466836754050},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1466836754054},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1466836754054},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1466836754050},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1466836754054},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1466836754054},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1466836754054},{"_id":"source/_posts/svm.md","hash":"9bb82e2d6bb26dc38f6ecac9d7c2e4c314d5ef20","modified":1466837650890},{"_id":"public/2016/06/25/hello-world/index.html","hash":"718adbc765c6f7bcf9d1231fab7aa95f5daea3a6","modified":1466837926895},{"_id":"public/archives/index.html","hash":"f35eec014475ceca2bcd6cf2326f421a4e917a4b","modified":1466838091328},{"_id":"public/archives/2016/index.html","hash":"bf05c683f3a680fb95dba5c1da3fef6ee997c59c","modified":1466838091329},{"_id":"public/archives/2016/06/index.html","hash":"499a7889eccbcd51ff0fbba666086f6b1ebf9527","modified":1466838091329},{"_id":"public/2016/06/25/svm/index.html","hash":"3aee6a34a1fbf5b41cfae8f75c780e93389dadae","modified":1466838091329},{"_id":"public/index.html","hash":"60c2c3c8ede4390e1f13ba71f5220cbd0139eea8","modified":1466838091329},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1466837705795},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1466837705797},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1466837705797},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1466837705797},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1466837705797},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1466837705797},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1466837705797},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1466837705797},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1466837705797},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1466837705798},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1466837706455},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1466837706459},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1466837706459},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1466837706459},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1466837706460},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1466837706460},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1466837706460},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1466837706460},{"_id":"public/css/style.css","hash":"11c9f812d2e2c9f4980553c78844522fa960a413","modified":1466837706460},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1466837706460},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1466837706460},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1466837706463},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1466837706463}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"suport vector machine","_content":"\n- [Introduction](#sec-1)\n- [Spark](#sec-2)\n  - [RDD](#sec-2-1)\n  - [Cluster mode](#sec-2-2)\n- [SVM](#sec-3)\n  - [Introduction](#sec-3-1)\n  - [SGD](#sec-3-2)\n  - [Pegasos and MLlib implementation](#sec-3-3)\n- [SGD in Spark](#sec-4)\n  - [treeAggregate](#sec-4-1)\n  - [Implementation](#sec-4-2)\n- [Experiments and Performance](#sec-5)\n  - [Experiments with small dataset](#sec-5-1)\n  - [The convergence speed](#sec-5-2)\n\n# Introduction<a id=\"orgheadline1\"></a>\n\nSpark is a fast and general engine for large-scale data processing. Spark is often compared with Hadoop. It claims that it is 100x faster than Hadoop MapReduce when computing in memory and 10x faster on disk. This speed is achieved because Spark handles computing in memory. With Hadoop MapReduce, the map reduce result is stored in disk and you have to read the result again from disk if you need it.\n But with Spark, the result is still in memory, which makes it every useful for iterative jobs that need to run map reduce many times.\n\nAnd we know that iteration is very common in machine learning algorithms, such as gradient descent. So we think spark\nwould be a good platform for large scale machine learning and we want to test its performence.\n\nSo with the out-standing performance of Spark, in this project we'd like to analyze the performance of machine learning tasks with Spark under TeraLab which is a Big Data analysis platform built by L'Institut Mines-Télécom and Le GENES, le Groupe des Ecoles nationales d’économie et de statistique.\n\n# Spark<a id=\"orgheadline4\"></a>\n\n## RDD<a id=\"orgheadline2\"></a>\n\nThe most important data structure of Spark is **RDD(Resilient Distributed Datasets)**, a distributed memory abstraction that we\ncan perform in-memory operations on large clusters in a fault-tolerant manner.\nEvery RDD has multiple partitions which are distributed among clusters. And these partitions are the computing unit.\n\nThere are 2 main types of operations: **transformation** and **action**.\n**Transformation** transforms one RDD from anthor one, such as *map* and *filter*.\n**Action** get results from RDD, such as *collect* and *reduce*.\nAnd one most important feature of RDD is the all the operations are lazy evaluated. That is to say that transformations won't be executed\nonly if the final action is met.\n\nFigure [9](#orgparagraph1) shows a simple graph of operations. *A* is the initial RDD which might be read from text file. Then transforms transform *A* to *B* and *C*. Finally, an action gets a normal Scala object such as *float*, *int* or *array* from *C*.\n\n![img](./imgs/rdd.png \"RDD transformations and actions\")\n\n## Cluster mode<a id=\"orgheadline3\"></a>\n\nFigure [11](#orgparagraph2) shows the basic architecture of Spark running in the cluster. In the cluster mode, a Spark application consists of a single **driver** process and a set of **executor** processes scattered across nodes on the cluster. The driver is the process that is in charge of the high-level control flow of work that needs to be done. The executor processes are responsible for executing this work, in the form of tasks, as well as for storing any data that the user chooses to cache.\n\n![img](./imgs/spark.png \"Spark cluster architecture(from ![img](//spark.apache.org/docs/latest/img/cluster-overview.png))\")\n\nAt the top of the execution hierarchy are **jobs**.\nAn action(collect, reduce&#x2026;) triggers the launch of a **job**.\nThen, Spark examines the graph of RDDs on which the action depends. Spark will find the farthest back RDDs that depend on no other RDDs or already cached. After finding the dependency, Spark puts the job's transformations into **stage**. Each stage includes a colllection of **tasks** that run the same code on each subset of data.\n\n\\newpage\n\n# SVM<a id=\"orgheadline8\"></a>\n\n## Introduction<a id=\"orgheadline5\"></a>\n\nSupport Vector Machine is an effective and popular classification learning tool.\n\nGiven a training set \\\\( S = \\{ (x_i, y_i) \\}_{i=1}^{m} \\\\), where \\\\( x_i \\in \\mathbb{R}^n \\\\) and \\\\( y_i \\in \\{ +1, -1 \\} \\\\), figure [22](#orgparagraph3) shows what a SVM looks like. We note  \\\\( w \\cdot x - b = 0 \\\\) as the hyperplane where \\\\( w \\\\) is the vector to the hyperplane. What we want is to find the maximum-margin hyperplane that divides the points having \\\\( y_i=1 \\\\) from those having \\\\( y_i=-1 \\\\). That means \\\\( y_i(w \\cdot x_i -b ) \\geq 1 \\\\) for all \\\\( 1 \\leq i \\leq n \\\\).\n\nSo the optimization problem can be written as\n\nMaximize\n\n\\\\[ \\frac{2}{\\|w\\|} \\\\]\n\nWhich equals to minimize\n\n\\\\[ \\frac{1}{2} \\| w \\|^2 \\\\]\n\nsubject to \\\\( y_i(w \\cdot x_i - b) \\geq 1 \\\\) for all \\\\( 1 \\leq i \\leq n \\\\)\n\n![img](./imgs/svm.png \"SVM\")\n\nIn fact, it can be viewed as an unconstrained empirical loss minimization with a penalty term for the norm of the classifier that is being learned. We would like to find the minimization of the problem\n\n\\\\[\n \\min_{w} \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x, y) \\in S} \\ell(w; (x, y))\n\\\\]\n\nWhere &lambda; is the regularization parameter, \\( \\ell(w, (x, y)) \\) is the hinge loss:\n\n\\\\[\n\\ell(w, (x, y)) = max\\{0, 1-y \\langle w, x \\rangle \\}\n\\\\]\n\nTo solve this optimization problem, we can use gradient descent to achieve the minimum value.\n\nThe objective function is\n\n\\\\[\nf(w) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))\n\\\\]\n\nThen, the sub-gradient for iteration *t* is\n\\\\[\n\\nabla_t = \\lambda w_t - \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i\n\\\\]\n\nNow we can update \\\\( w \\\\), where \\\\( \\eta_t \\\\) is the step size\n\\\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\\\]\n\n## SGD<a id=\"orgheadline6\"></a>\n\nFrom the previous section, we notice that we need to iterate all the data point when calculating gradient.\nAnd this might be computing expensive if we have tons of data. This is the reason why Stochastic Gradient Descent(SGD) becomes so useful.\nWhen handling large scale problems, SGD uses sub dataset at each iteration instead of the whole dataset.\n\nSo now, the object function becomes:\n\\\\[\nf(w, A_t) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))\n\\\\]\nwhere \\( A_t \\subset S \\), \\( |A_t| = k \\). At each iteration, we takes a subset of data point.\n\nAnd sub-gradient is\n \\\\[ \\nabla_t = \\lambda w_t - \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i \\\\]\n\n## Pegasos and MLlib implementation<a id=\"orgheadline7\"></a>\n\nPegasos, is a kind of stochastic gradient descent algorithm. And Spark MLlib also provides an SGD implementation for us. After reading the code of MLLib, we notice that the only difference between Pegasos and MLlib is the choice of update step.\n\n\\\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\\\]\n\nIn Pegasos, the update step is\n\\\\[\n\\eta_t = \\frac{\\alpha}{t\\lambda}\n\\\\]\n\nIn MLlib, this is\n\\\\[\n\\eta_t = \\frac{\\alpha}{\\sqrt{t}}\n\\\\]\n\nwhere &alpha; is the step size parameter\n\n# SGD in Spark<a id=\"orgheadline11\"></a>\n\n## treeAggregate<a id=\"orgheadline9\"></a>\n\nThe main usage of Spark for SGD is to calculate the gradient which need to sum up the value of every data point. And in Spark, this is done by the RDD method **treeAggregate**. **Aggregate** is a generalized combination of **Map** and **Reduce**. The definition of **treeAggregate** is\n\n```scala\nRDD.treeAggregate(zeroValue: U)(\n      seqOp: (U, T) => U,\n      combOp: (U, U) => U,\n      depth: Int = 2): U\n```\n\nIn this method, there are three parameters in which the first two are more important for us.\n\n-   seqOp: calculate sub gradient for every partition\n-   combOp: combine the result of seqOp or upper level combOp together\n-   depth: control the depth of the aggregation tree\n\n![img](./imgs/tree.png \"tree aggregate\")\n\nWe can see from the figure [46](#orgparagraph4) that the first thing is to use the **seqOp** to calculate the sub gradient for every partition, then it sums them up level by level using **combOp**.\n\n## Implementation<a id=\"orgheadline10\"></a>\n\nIn this section, the code for the main SGD logic is shown in [2](#orgsrcblock1). It runs `numIterations` times to get the final \\( w \\).\n\nFirst, `data.sample` takes a subset of data whose size is decided by `miniBatchFraction`. Then we use `treeAggregate` method on this sample. In `seqOp`, The `gradientSum` is updated by `axpy(y, b_x, c._1)` if \\( y\\langle w, x \\rangle < 1 \\) which means wrong classification. In `combOp`, `gradientSum` is combined together by `c1._1 += c2._1`. After we get the `gradientSum`, we calcuate `step` and `gradient`. Finally, we update the weights with `axpy(-step, gradient, weights)`\n\n```scala\nfor (i <- 1 to numIterations) {\n      val bcWeights = sc.broadcast(weights)\n\n  val (gradientSum, lossSum, batchSize) = data.sample(false,\n    miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](weights.size), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val y = v.label\n            val x = v.features\n            val b_x = BDV(x.toArray)\n            val dotProduct = bcWeights.value.dot(b_x)\n            if (y * dotProduct < 1) {\n              axpy(y, b_x, c._1)    // add to gradientSum\n            }\n            (c._1, c._2 + math.max(0, 1 - y * dotProduct), c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      val step = stepSize / (regParam * i)\n      val gradient = weights * regParam - gradientSum / batchSize.toDouble\n      axpy(-step, gradient, weights)    // update weights\n    }\n```\n\n\\newpage\n\n# Experiments and Performance<a id=\"orgheadline15\"></a>\n\n## Experiments with small dataset<a id=\"orgheadline12\"></a>\n\nThe first thing we need to do is to show that our Pegasos implementation works correctly. To achieve that, we simulate some sample 2D and 3D data with normal distribution.\nThe first one is a 2D linear dataset. The result is in figure [52](#orgparagraph5).\n\n![img](./imgs/2d_linear.png \"2D linear\")\n\nThen this figure [54](#orgparagraph6) shows the result of a 3D linear dataset.\n\n![img](./imgs/3d_linear.png \"3D linear\")\n\nFrom those experiments, we show that our Pegasos implementation works well. Then we will test it's performance with larger dataset.\n\n## The convergence speed<a id=\"orgheadline13\"></a>\n\nSince the implementation of Pegasos and MLlib is slightly different. we would like to compare the convergence speed of Pegasos and MLlib. In this test, we take 5GB data with 1000 features, launch the job with 4 executors and run 100 iterations. The result is in figure [57](#orgparagraph7), where the Y axis is not aligned. In the plot, the first 30 iterations are ignored since the initial loss is too high.\n\n<./imgs/step1.eps>\n\nThen we align the Y axis in figure [59](#orgparagraph8). From those 2 figures, we can see that when the step size is well chosen, Pegasos and MLlib have similar performance. But Pegasos has one advantage that it is easier to find the right step parameter. In most cases, 1 is good for Pegasos.\n\n<./imgs/step2.eps>\n\n# Reference<a id=\"orgheadline18\"></a>\n\n1.  Zaharia, Matei, et al. \"Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.\" Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012\n2.  Zaharia, Matei, et al. \"Spark: cluster computing with working sets.\" Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010\n3.  Shalev-Shwartz, Shai, et al. \"Pegasos: Primal estimated sub-gradient solver for svm.\" Mathematical programming 127.1 (2011): 3-30\n","source":"_posts/svm.md","raw":"---\ntitle:  suport vector machine\n---\n\n- [Introduction](#sec-1)\n- [Spark](#sec-2)\n  - [RDD](#sec-2-1)\n  - [Cluster mode](#sec-2-2)\n- [SVM](#sec-3)\n  - [Introduction](#sec-3-1)\n  - [SGD](#sec-3-2)\n  - [Pegasos and MLlib implementation](#sec-3-3)\n- [SGD in Spark](#sec-4)\n  - [treeAggregate](#sec-4-1)\n  - [Implementation](#sec-4-2)\n- [Experiments and Performance](#sec-5)\n  - [Experiments with small dataset](#sec-5-1)\n  - [The convergence speed](#sec-5-2)\n\n# Introduction<a id=\"orgheadline1\"></a>\n\nSpark is a fast and general engine for large-scale data processing. Spark is often compared with Hadoop. It claims that it is 100x faster than Hadoop MapReduce when computing in memory and 10x faster on disk. This speed is achieved because Spark handles computing in memory. With Hadoop MapReduce, the map reduce result is stored in disk and you have to read the result again from disk if you need it.\n But with Spark, the result is still in memory, which makes it every useful for iterative jobs that need to run map reduce many times.\n\nAnd we know that iteration is very common in machine learning algorithms, such as gradient descent. So we think spark\nwould be a good platform for large scale machine learning and we want to test its performence.\n\nSo with the out-standing performance of Spark, in this project we'd like to analyze the performance of machine learning tasks with Spark under TeraLab which is a Big Data analysis platform built by L'Institut Mines-Télécom and Le GENES, le Groupe des Ecoles nationales d’économie et de statistique.\n\n# Spark<a id=\"orgheadline4\"></a>\n\n## RDD<a id=\"orgheadline2\"></a>\n\nThe most important data structure of Spark is **RDD(Resilient Distributed Datasets)**, a distributed memory abstraction that we\ncan perform in-memory operations on large clusters in a fault-tolerant manner.\nEvery RDD has multiple partitions which are distributed among clusters. And these partitions are the computing unit.\n\nThere are 2 main types of operations: **transformation** and **action**.\n**Transformation** transforms one RDD from anthor one, such as *map* and *filter*.\n**Action** get results from RDD, such as *collect* and *reduce*.\nAnd one most important feature of RDD is the all the operations are lazy evaluated. That is to say that transformations won't be executed\nonly if the final action is met.\n\nFigure [9](#orgparagraph1) shows a simple graph of operations. *A* is the initial RDD which might be read from text file. Then transforms transform *A* to *B* and *C*. Finally, an action gets a normal Scala object such as *float*, *int* or *array* from *C*.\n\n![img](./imgs/rdd.png \"RDD transformations and actions\")\n\n## Cluster mode<a id=\"orgheadline3\"></a>\n\nFigure [11](#orgparagraph2) shows the basic architecture of Spark running in the cluster. In the cluster mode, a Spark application consists of a single **driver** process and a set of **executor** processes scattered across nodes on the cluster. The driver is the process that is in charge of the high-level control flow of work that needs to be done. The executor processes are responsible for executing this work, in the form of tasks, as well as for storing any data that the user chooses to cache.\n\n![img](./imgs/spark.png \"Spark cluster architecture(from ![img](//spark.apache.org/docs/latest/img/cluster-overview.png))\")\n\nAt the top of the execution hierarchy are **jobs**.\nAn action(collect, reduce&#x2026;) triggers the launch of a **job**.\nThen, Spark examines the graph of RDDs on which the action depends. Spark will find the farthest back RDDs that depend on no other RDDs or already cached. After finding the dependency, Spark puts the job's transformations into **stage**. Each stage includes a colllection of **tasks** that run the same code on each subset of data.\n\n\\newpage\n\n# SVM<a id=\"orgheadline8\"></a>\n\n## Introduction<a id=\"orgheadline5\"></a>\n\nSupport Vector Machine is an effective and popular classification learning tool.\n\nGiven a training set \\\\( S = \\{ (x_i, y_i) \\}_{i=1}^{m} \\\\), where \\\\( x_i \\in \\mathbb{R}^n \\\\) and \\\\( y_i \\in \\{ +1, -1 \\} \\\\), figure [22](#orgparagraph3) shows what a SVM looks like. We note  \\\\( w \\cdot x - b = 0 \\\\) as the hyperplane where \\\\( w \\\\) is the vector to the hyperplane. What we want is to find the maximum-margin hyperplane that divides the points having \\\\( y_i=1 \\\\) from those having \\\\( y_i=-1 \\\\). That means \\\\( y_i(w \\cdot x_i -b ) \\geq 1 \\\\) for all \\\\( 1 \\leq i \\leq n \\\\).\n\nSo the optimization problem can be written as\n\nMaximize\n\n\\\\[ \\frac{2}{\\|w\\|} \\\\]\n\nWhich equals to minimize\n\n\\\\[ \\frac{1}{2} \\| w \\|^2 \\\\]\n\nsubject to \\\\( y_i(w \\cdot x_i - b) \\geq 1 \\\\) for all \\\\( 1 \\leq i \\leq n \\\\)\n\n![img](./imgs/svm.png \"SVM\")\n\nIn fact, it can be viewed as an unconstrained empirical loss minimization with a penalty term for the norm of the classifier that is being learned. We would like to find the minimization of the problem\n\n\\\\[\n \\min_{w} \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x, y) \\in S} \\ell(w; (x, y))\n\\\\]\n\nWhere &lambda; is the regularization parameter, \\( \\ell(w, (x, y)) \\) is the hinge loss:\n\n\\\\[\n\\ell(w, (x, y)) = max\\{0, 1-y \\langle w, x \\rangle \\}\n\\\\]\n\nTo solve this optimization problem, we can use gradient descent to achieve the minimum value.\n\nThe objective function is\n\n\\\\[\nf(w) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))\n\\\\]\n\nThen, the sub-gradient for iteration *t* is\n\\\\[\n\\nabla_t = \\lambda w_t - \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i\n\\\\]\n\nNow we can update \\\\( w \\\\), where \\\\( \\eta_t \\\\) is the step size\n\\\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\\\]\n\n## SGD<a id=\"orgheadline6\"></a>\n\nFrom the previous section, we notice that we need to iterate all the data point when calculating gradient.\nAnd this might be computing expensive if we have tons of data. This is the reason why Stochastic Gradient Descent(SGD) becomes so useful.\nWhen handling large scale problems, SGD uses sub dataset at each iteration instead of the whole dataset.\n\nSo now, the object function becomes:\n\\\\[\nf(w, A_t) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))\n\\\\]\nwhere \\( A_t \\subset S \\), \\( |A_t| = k \\). At each iteration, we takes a subset of data point.\n\nAnd sub-gradient is\n \\\\[ \\nabla_t = \\lambda w_t - \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i \\\\]\n\n## Pegasos and MLlib implementation<a id=\"orgheadline7\"></a>\n\nPegasos, is a kind of stochastic gradient descent algorithm. And Spark MLlib also provides an SGD implementation for us. After reading the code of MLLib, we notice that the only difference between Pegasos and MLlib is the choice of update step.\n\n\\\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\\\]\n\nIn Pegasos, the update step is\n\\\\[\n\\eta_t = \\frac{\\alpha}{t\\lambda}\n\\\\]\n\nIn MLlib, this is\n\\\\[\n\\eta_t = \\frac{\\alpha}{\\sqrt{t}}\n\\\\]\n\nwhere &alpha; is the step size parameter\n\n# SGD in Spark<a id=\"orgheadline11\"></a>\n\n## treeAggregate<a id=\"orgheadline9\"></a>\n\nThe main usage of Spark for SGD is to calculate the gradient which need to sum up the value of every data point. And in Spark, this is done by the RDD method **treeAggregate**. **Aggregate** is a generalized combination of **Map** and **Reduce**. The definition of **treeAggregate** is\n\n```scala\nRDD.treeAggregate(zeroValue: U)(\n      seqOp: (U, T) => U,\n      combOp: (U, U) => U,\n      depth: Int = 2): U\n```\n\nIn this method, there are three parameters in which the first two are more important for us.\n\n-   seqOp: calculate sub gradient for every partition\n-   combOp: combine the result of seqOp or upper level combOp together\n-   depth: control the depth of the aggregation tree\n\n![img](./imgs/tree.png \"tree aggregate\")\n\nWe can see from the figure [46](#orgparagraph4) that the first thing is to use the **seqOp** to calculate the sub gradient for every partition, then it sums them up level by level using **combOp**.\n\n## Implementation<a id=\"orgheadline10\"></a>\n\nIn this section, the code for the main SGD logic is shown in [2](#orgsrcblock1). It runs `numIterations` times to get the final \\( w \\).\n\nFirst, `data.sample` takes a subset of data whose size is decided by `miniBatchFraction`. Then we use `treeAggregate` method on this sample. In `seqOp`, The `gradientSum` is updated by `axpy(y, b_x, c._1)` if \\( y\\langle w, x \\rangle < 1 \\) which means wrong classification. In `combOp`, `gradientSum` is combined together by `c1._1 += c2._1`. After we get the `gradientSum`, we calcuate `step` and `gradient`. Finally, we update the weights with `axpy(-step, gradient, weights)`\n\n```scala\nfor (i <- 1 to numIterations) {\n      val bcWeights = sc.broadcast(weights)\n\n  val (gradientSum, lossSum, batchSize) = data.sample(false,\n    miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](weights.size), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val y = v.label\n            val x = v.features\n            val b_x = BDV(x.toArray)\n            val dotProduct = bcWeights.value.dot(b_x)\n            if (y * dotProduct < 1) {\n              axpy(y, b_x, c._1)    // add to gradientSum\n            }\n            (c._1, c._2 + math.max(0, 1 - y * dotProduct), c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      val step = stepSize / (regParam * i)\n      val gradient = weights * regParam - gradientSum / batchSize.toDouble\n      axpy(-step, gradient, weights)    // update weights\n    }\n```\n\n\\newpage\n\n# Experiments and Performance<a id=\"orgheadline15\"></a>\n\n## Experiments with small dataset<a id=\"orgheadline12\"></a>\n\nThe first thing we need to do is to show that our Pegasos implementation works correctly. To achieve that, we simulate some sample 2D and 3D data with normal distribution.\nThe first one is a 2D linear dataset. The result is in figure [52](#orgparagraph5).\n\n![img](./imgs/2d_linear.png \"2D linear\")\n\nThen this figure [54](#orgparagraph6) shows the result of a 3D linear dataset.\n\n![img](./imgs/3d_linear.png \"3D linear\")\n\nFrom those experiments, we show that our Pegasos implementation works well. Then we will test it's performance with larger dataset.\n\n## The convergence speed<a id=\"orgheadline13\"></a>\n\nSince the implementation of Pegasos and MLlib is slightly different. we would like to compare the convergence speed of Pegasos and MLlib. In this test, we take 5GB data with 1000 features, launch the job with 4 executors and run 100 iterations. The result is in figure [57](#orgparagraph7), where the Y axis is not aligned. In the plot, the first 30 iterations are ignored since the initial loss is too high.\n\n<./imgs/step1.eps>\n\nThen we align the Y axis in figure [59](#orgparagraph8). From those 2 figures, we can see that when the step size is well chosen, Pegasos and MLlib have similar performance. But Pegasos has one advantage that it is easier to find the right step parameter. In most cases, 1 is good for Pegasos.\n\n<./imgs/step2.eps>\n\n# Reference<a id=\"orgheadline18\"></a>\n\n1.  Zaharia, Matei, et al. \"Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.\" Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012\n2.  Zaharia, Matei, et al. \"Spark: cluster computing with working sets.\" Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010\n3.  Shalev-Shwartz, Shai, et al. \"Pegasos: Primal estimated sub-gradient solver for svm.\" Mathematical programming 127.1 (2011): 3-30\n","slug":"svm","published":1,"date":"2016-06-25T06:54:10.946Z","updated":"2016-06-25T06:54:10.890Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciput9qut0000pbw6wi1gm2vk","content":"<ul>\n<li><a href=\"#sec-1\">Introduction</a></li>\n<li><a href=\"#sec-2\">Spark</a><ul>\n<li><a href=\"#sec-2-1\">RDD</a></li>\n<li><a href=\"#sec-2-2\">Cluster mode</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-3\">SVM</a><ul>\n<li><a href=\"#sec-3-1\">Introduction</a></li>\n<li><a href=\"#sec-3-2\">SGD</a></li>\n<li><a href=\"#sec-3-3\">Pegasos and MLlib implementation</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-4\">SGD in Spark</a><ul>\n<li><a href=\"#sec-4-1\">treeAggregate</a></li>\n<li><a href=\"#sec-4-2\">Implementation</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-5\">Experiments and Performance</a><ul>\n<li><a href=\"#sec-5-1\">Experiments with small dataset</a></li>\n<li><a href=\"#sec-5-2\">The convergence speed</a></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction<a id=\"orgheadline1\"></a></h1><p>Spark is a fast and general engine for large-scale data processing. Spark is often compared with Hadoop. It claims that it is 100x faster than Hadoop MapReduce when computing in memory and 10x faster on disk. This speed is achieved because Spark handles computing in memory. With Hadoop MapReduce, the map reduce result is stored in disk and you have to read the result again from disk if you need it.<br> But with Spark, the result is still in memory, which makes it every useful for iterative jobs that need to run map reduce many times.</p>\n<p>And we know that iteration is very common in machine learning algorithms, such as gradient descent. So we think spark<br>would be a good platform for large scale machine learning and we want to test its performence.</p>\n<p>So with the out-standing performance of Spark, in this project we’d like to analyze the performance of machine learning tasks with Spark under TeraLab which is a Big Data analysis platform built by L’Institut Mines-Télécom and Le GENES, le Groupe des Ecoles nationales d’économie et de statistique.</p>\n<h1 id=\"Spark\"><a href=\"#Spark\" class=\"headerlink\" title=\"Spark\"></a>Spark<a id=\"orgheadline4\"></a></h1><h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD<a id=\"orgheadline2\"></a></h2><p>The most important data structure of Spark is <strong>RDD(Resilient Distributed Datasets)</strong>, a distributed memory abstraction that we<br>can perform in-memory operations on large clusters in a fault-tolerant manner.<br>Every RDD has multiple partitions which are distributed among clusters. And these partitions are the computing unit.</p>\n<p>There are 2 main types of operations: <strong>transformation</strong> and <strong>action</strong>.<br><strong>Transformation</strong> transforms one RDD from anthor one, such as <em>map</em> and <em>filter</em>.<br><strong>Action</strong> get results from RDD, such as <em>collect</em> and <em>reduce</em>.<br>And one most important feature of RDD is the all the operations are lazy evaluated. That is to say that transformations won’t be executed<br>only if the final action is met.</p>\n<p>Figure <a href=\"#orgparagraph1\">9</a> shows a simple graph of operations. <em>A</em> is the initial RDD which might be read from text file. Then transforms transform <em>A</em> to <em>B</em> and <em>C</em>. Finally, an action gets a normal Scala object such as <em>float</em>, <em>int</em> or <em>array</em> from <em>C</em>.</p>\n<p><img src=\"./imgs/rdd.png\" alt=\"img\" title=\"RDD transformations and actions\"></p>\n<h2 id=\"Cluster-mode\"><a href=\"#Cluster-mode\" class=\"headerlink\" title=\"Cluster mode\"></a>Cluster mode<a id=\"orgheadline3\"></a></h2><p>Figure <a href=\"#orgparagraph2\">11</a> shows the basic architecture of Spark running in the cluster. In the cluster mode, a Spark application consists of a single <strong>driver</strong> process and a set of <strong>executor</strong> processes scattered across nodes on the cluster. The driver is the process that is in charge of the high-level control flow of work that needs to be done. The executor processes are responsible for executing this work, in the form of tasks, as well as for storing any data that the user chooses to cache.</p>\n<p><img src=\"./imgs/spark.png\" alt=\"img\" title=\"Spark cluster architecture(from ![img](//spark.apache.org/docs/latest/img/cluster-overview.png))\"></p>\n<p>At the top of the execution hierarchy are <strong>jobs</strong>.<br>An action(collect, reduce&#x2026;) triggers the launch of a <strong>job</strong>.<br>Then, Spark examines the graph of RDDs on which the action depends. Spark will find the farthest back RDDs that depend on no other RDDs or already cached. After finding the dependency, Spark puts the job’s transformations into <strong>stage</strong>. Each stage includes a colllection of <strong>tasks</strong> that run the same code on each subset of data.</p>\n<p>\\newpage</p>\n<h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM<a id=\"orgheadline8\"></a></h1><h2 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction<a id=\"orgheadline5\"></a></h2><p>Support Vector Machine is an effective and popular classification learning tool.</p>\n<p>Given a training set \\( S = { (x_i, y<em>i) }</em>{i=1}^{m} \\), where \\( x_i \\in \\mathbb{R}^n \\) and \\( y_i \\in { +1, -1 } \\), figure <a href=\"#orgparagraph3\">22</a> shows what a SVM looks like. We note  \\( w \\cdot x - b = 0 \\) as the hyperplane where \\( w \\) is the vector to the hyperplane. What we want is to find the maximum-margin hyperplane that divides the points having \\( y_i=1 \\) from those having \\( y_i=-1 \\). That means \\( y_i(w \\cdot x_i -b ) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\).</p>\n<p>So the optimization problem can be written as</p>\n<p>Maximize</p>\n<p>\\[ \\frac{2}{|w|} \\]</p>\n<p>Which equals to minimize</p>\n<p>\\[ \\frac{1}{2} | w |^2 \\]</p>\n<p>subject to \\( y_i(w \\cdot x_i - b) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\)</p>\n<p><img src=\"./imgs/svm.png\" alt=\"img\" title=\"SVM\"></p>\n<p>In fact, it can be viewed as an unconstrained empirical loss minimization with a penalty term for the norm of the classifier that is being learned. We would like to find the minimization of the problem</p>\n<p>\\[<br> \\min<em>{w} \\frac{\\lambda}{2}|w|^2 + \\frac{1}{m}\\sum</em>{(x, y) \\in S} \\ell(w; (x, y))<br>\\]</p>\n<p>Where &lambda; is the regularization parameter, ( \\ell(w, (x, y)) ) is the hinge loss:</p>\n<p>\\[<br>\\ell(w, (x, y)) = max{0, 1-y \\langle w, x \\rangle }<br>\\]</p>\n<p>To solve this optimization problem, we can use gradient descent to achieve the minimum value.</p>\n<p>The objective function is</p>\n<p>\\[<br>f(w) = \\frac{\\lambda}{2}|w|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))<br>\\]</p>\n<p>Then, the sub-gradient for iteration <em>t</em> is<br>\\[<br>\\nabla_t = \\lambda w<em>t - \\frac{1}{m}\\sum</em>{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i<br>\\]</p>\n<p>Now we can update \\( w \\), where \\( \\eta<em>t \\) is the step size<br>\\[<br>w</em>{t+1} \\leftarrow w_t - \\eta_t\\nabla_t<br>\\]</p>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD<a id=\"orgheadline6\"></a></h2><p>From the previous section, we notice that we need to iterate all the data point when calculating gradient.<br>And this might be computing expensive if we have tons of data. This is the reason why Stochastic Gradient Descent(SGD) becomes so useful.<br>When handling large scale problems, SGD uses sub dataset at each iteration instead of the whole dataset.</p>\n<p>So now, the object function becomes:<br>\\[<br>f(w, A<em>t) = \\frac{\\lambda}{2}|w|^2 + \\frac{1}{k}\\sum</em>{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))<br>\\]<br>where ( A_t \\subset S ), ( |A_t| = k ). At each iteration, we takes a subset of data point.</p>\n<p>And sub-gradient is<br> \\[ \\nabla_t = \\lambda w<em>t - \\frac{1}{k}\\sum</em>{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i \\]</p>\n<h2 id=\"Pegasos-and-MLlib-implementation\"><a href=\"#Pegasos-and-MLlib-implementation\" class=\"headerlink\" title=\"Pegasos and MLlib implementation\"></a>Pegasos and MLlib implementation<a id=\"orgheadline7\"></a></h2><p>Pegasos, is a kind of stochastic gradient descent algorithm. And Spark MLlib also provides an SGD implementation for us. After reading the code of MLLib, we notice that the only difference between Pegasos and MLlib is the choice of update step.</p>\n<p>\\[<br>w_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t<br>\\]</p>\n<p>In Pegasos, the update step is<br>\\[<br>\\eta_t = \\frac{\\alpha}{t\\lambda}<br>\\]</p>\n<p>In MLlib, this is<br>\\[<br>\\eta_t = \\frac{\\alpha}{\\sqrt{t}}<br>\\]</p>\n<p>where &alpha; is the step size parameter</p>\n<h1 id=\"SGD-in-Spark\"><a href=\"#SGD-in-Spark\" class=\"headerlink\" title=\"SGD in Spark\"></a>SGD in Spark<a id=\"orgheadline11\"></a></h1><h2 id=\"treeAggregate\"><a href=\"#treeAggregate\" class=\"headerlink\" title=\"treeAggregate\"></a>treeAggregate<a id=\"orgheadline9\"></a></h2><p>The main usage of Spark for SGD is to calculate the gradient which need to sum up the value of every data point. And in Spark, this is done by the RDD method <strong>treeAggregate</strong>. <strong>Aggregate</strong> is a generalized combination of <strong>Map</strong> and <strong>Reduce</strong>. The definition of <strong>treeAggregate</strong> is</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">RDD</span>.treeAggregate(zeroValue: <span class=\"type\">U</span>)(</span><br><span class=\"line\">      seqOp: (<span class=\"type\">U</span>, <span class=\"type\">T</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      combOp: (<span class=\"type\">U</span>, <span class=\"type\">U</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      depth: <span class=\"type\">Int</span> = <span class=\"number\">2</span>): <span class=\"type\">U</span></span><br></pre></td></tr></table></figure>\n<p>In this method, there are three parameters in which the first two are more important for us.</p>\n<ul>\n<li>seqOp: calculate sub gradient for every partition</li>\n<li>combOp: combine the result of seqOp or upper level combOp together</li>\n<li>depth: control the depth of the aggregation tree</li>\n</ul>\n<p><img src=\"./imgs/tree.png\" alt=\"img\" title=\"tree aggregate\"></p>\n<p>We can see from the figure <a href=\"#orgparagraph4\">46</a> that the first thing is to use the <strong>seqOp</strong> to calculate the sub gradient for every partition, then it sums them up level by level using <strong>combOp</strong>.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation<a id=\"orgheadline10\"></a></h2><p>In this section, the code for the main SGD logic is shown in <a href=\"#orgsrcblock1\">2</a>. It runs <code>numIterations</code> times to get the final ( w ).</p>\n<p>First, <code>data.sample</code> takes a subset of data whose size is decided by <code>miniBatchFraction</code>. Then we use <code>treeAggregate</code> method on this sample. In <code>seqOp</code>, The <code>gradientSum</code> is updated by <code>axpy(y, b_x, c._1)</code> if ( y\\langle w, x \\rangle &lt; 1 ) which means wrong classification. In <code>combOp</code>, <code>gradientSum</code> is combined together by <code>c1._1 += c2._1</code>. After we get the <code>gradientSum</code>, we calcuate <code>step</code> and <code>gradient</code>. Finally, we update the weights with <code>axpy(-step, gradient, weights)</code></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to numIterations) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> bcWeights = sc.broadcast(weights)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> (gradientSum, lossSum, batchSize) = data.sample(<span class=\"literal\">false</span>,</span><br><span class=\"line\">    miniBatchFraction, <span class=\"number\">42</span> + i)</span><br><span class=\"line\">        .treeAggregate((<span class=\"type\">BDV</span>.zeros[<span class=\"type\">Double</span>](weights.size), <span class=\"number\">0.0</span>, <span class=\"number\">0</span>L))(</span><br><span class=\"line\">          seqOp = (c, v) =&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">// c: (grad, loss, count), v: (label, features)</span></span><br><span class=\"line\">            <span class=\"keyword\">val</span> y = v.label</span><br><span class=\"line\">            <span class=\"keyword\">val</span> x = v.features</span><br><span class=\"line\">            <span class=\"keyword\">val</span> b_x = <span class=\"type\">BDV</span>(x.toArray)</span><br><span class=\"line\">            <span class=\"keyword\">val</span> dotProduct = bcWeights.value.dot(b_x)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (y * dotProduct &lt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">              axpy(y, b_x, c._1)    <span class=\"comment\">// add to gradientSum</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            (c._1, c._2 + math.max(<span class=\"number\">0</span>, <span class=\"number\">1</span> - y * dotProduct), c._3 + <span class=\"number\">1</span>)</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          combOp = (c1, c2) =&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">// c: (grad, loss, count)</span></span><br><span class=\"line\">            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class=\"line\">          &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> step = stepSize / (regParam * i)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> gradient = weights * regParam - gradientSum / batchSize.toDouble</span><br><span class=\"line\">      axpy(-step, gradient, weights)    <span class=\"comment\">// update weights</span></span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>\\newpage</p>\n<h1 id=\"Experiments-and-Performance\"><a href=\"#Experiments-and-Performance\" class=\"headerlink\" title=\"Experiments and Performance\"></a>Experiments and Performance<a id=\"orgheadline15\"></a></h1><h2 id=\"Experiments-with-small-dataset\"><a href=\"#Experiments-with-small-dataset\" class=\"headerlink\" title=\"Experiments with small dataset\"></a>Experiments with small dataset<a id=\"orgheadline12\"></a></h2><p>The first thing we need to do is to show that our Pegasos implementation works correctly. To achieve that, we simulate some sample 2D and 3D data with normal distribution.<br>The first one is a 2D linear dataset. The result is in figure <a href=\"#orgparagraph5\">52</a>.</p>\n<p><img src=\"./imgs/2d_linear.png\" alt=\"img\" title=\"2D linear\"></p>\n<p>Then this figure <a href=\"#orgparagraph6\">54</a> shows the result of a 3D linear dataset.</p>\n<p><img src=\"./imgs/3d_linear.png\" alt=\"img\" title=\"3D linear\"></p>\n<p>From those experiments, we show that our Pegasos implementation works well. Then we will test it’s performance with larger dataset.</p>\n<h2 id=\"The-convergence-speed\"><a href=\"#The-convergence-speed\" class=\"headerlink\" title=\"The convergence speed\"></a>The convergence speed<a id=\"orgheadline13\"></a></h2><p>Since the implementation of Pegasos and MLlib is slightly different. we would like to compare the convergence speed of Pegasos and MLlib. In this test, we take 5GB data with 1000 features, launch the job with 4 executors and run 100 iterations. The result is in figure <a href=\"#orgparagraph7\">57</a>, where the Y axis is not aligned. In the plot, the first 30 iterations are ignored since the initial loss is too high.</p>\n<p>&lt;./imgs/step1.eps&gt;</p>\n<p>Then we align the Y axis in figure <a href=\"#orgparagraph8\">59</a>. From those 2 figures, we can see that when the step size is well chosen, Pegasos and MLlib have similar performance. But Pegasos has one advantage that it is easier to find the right step parameter. In most cases, 1 is good for Pegasos.</p>\n<p>&lt;./imgs/step2.eps&gt;</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference<a id=\"orgheadline18\"></a></h1><ol>\n<li>Zaharia, Matei, et al. “Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.” Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012</li>\n<li>Zaharia, Matei, et al. “Spark: cluster computing with working sets.” Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010</li>\n<li>Shalev-Shwartz, Shai, et al. “Pegasos: Primal estimated sub-gradient solver for svm.” Mathematical programming 127.1 (2011): 3-30</li>\n</ol>\n","excerpt":"","more":"<ul>\n<li><a href=\"#sec-1\">Introduction</a></li>\n<li><a href=\"#sec-2\">Spark</a><ul>\n<li><a href=\"#sec-2-1\">RDD</a></li>\n<li><a href=\"#sec-2-2\">Cluster mode</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-3\">SVM</a><ul>\n<li><a href=\"#sec-3-1\">Introduction</a></li>\n<li><a href=\"#sec-3-2\">SGD</a></li>\n<li><a href=\"#sec-3-3\">Pegasos and MLlib implementation</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-4\">SGD in Spark</a><ul>\n<li><a href=\"#sec-4-1\">treeAggregate</a></li>\n<li><a href=\"#sec-4-2\">Implementation</a></li>\n</ul>\n</li>\n<li><a href=\"#sec-5\">Experiments and Performance</a><ul>\n<li><a href=\"#sec-5-1\">Experiments with small dataset</a></li>\n<li><a href=\"#sec-5-2\">The convergence speed</a></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction<a id=\"orgheadline1\"></a></h1><p>Spark is a fast and general engine for large-scale data processing. Spark is often compared with Hadoop. It claims that it is 100x faster than Hadoop MapReduce when computing in memory and 10x faster on disk. This speed is achieved because Spark handles computing in memory. With Hadoop MapReduce, the map reduce result is stored in disk and you have to read the result again from disk if you need it.<br> But with Spark, the result is still in memory, which makes it every useful for iterative jobs that need to run map reduce many times.</p>\n<p>And we know that iteration is very common in machine learning algorithms, such as gradient descent. So we think spark<br>would be a good platform for large scale machine learning and we want to test its performence.</p>\n<p>So with the out-standing performance of Spark, in this project we’d like to analyze the performance of machine learning tasks with Spark under TeraLab which is a Big Data analysis platform built by L’Institut Mines-Télécom and Le GENES, le Groupe des Ecoles nationales d’économie et de statistique.</p>\n<h1 id=\"Spark\"><a href=\"#Spark\" class=\"headerlink\" title=\"Spark\"></a>Spark<a id=\"orgheadline4\"></a></h1><h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD<a id=\"orgheadline2\"></a></h2><p>The most important data structure of Spark is <strong>RDD(Resilient Distributed Datasets)</strong>, a distributed memory abstraction that we<br>can perform in-memory operations on large clusters in a fault-tolerant manner.<br>Every RDD has multiple partitions which are distributed among clusters. And these partitions are the computing unit.</p>\n<p>There are 2 main types of operations: <strong>transformation</strong> and <strong>action</strong>.<br><strong>Transformation</strong> transforms one RDD from anthor one, such as <em>map</em> and <em>filter</em>.<br><strong>Action</strong> get results from RDD, such as <em>collect</em> and <em>reduce</em>.<br>And one most important feature of RDD is the all the operations are lazy evaluated. That is to say that transformations won’t be executed<br>only if the final action is met.</p>\n<p>Figure <a href=\"#orgparagraph1\">9</a> shows a simple graph of operations. <em>A</em> is the initial RDD which might be read from text file. Then transforms transform <em>A</em> to <em>B</em> and <em>C</em>. Finally, an action gets a normal Scala object such as <em>float</em>, <em>int</em> or <em>array</em> from <em>C</em>.</p>\n<p><img src=\"./imgs/rdd.png\" alt=\"img\" title=\"RDD transformations and actions\"></p>\n<h2 id=\"Cluster-mode\"><a href=\"#Cluster-mode\" class=\"headerlink\" title=\"Cluster mode\"></a>Cluster mode<a id=\"orgheadline3\"></a></h2><p>Figure <a href=\"#orgparagraph2\">11</a> shows the basic architecture of Spark running in the cluster. In the cluster mode, a Spark application consists of a single <strong>driver</strong> process and a set of <strong>executor</strong> processes scattered across nodes on the cluster. The driver is the process that is in charge of the high-level control flow of work that needs to be done. The executor processes are responsible for executing this work, in the form of tasks, as well as for storing any data that the user chooses to cache.</p>\n<p><img src=\"./imgs/spark.png\" alt=\"img\" title=\"Spark cluster architecture(from ![img](//spark.apache.org/docs/latest/img/cluster-overview.png))\"></p>\n<p>At the top of the execution hierarchy are <strong>jobs</strong>.<br>An action(collect, reduce&#x2026;) triggers the launch of a <strong>job</strong>.<br>Then, Spark examines the graph of RDDs on which the action depends. Spark will find the farthest back RDDs that depend on no other RDDs or already cached. After finding the dependency, Spark puts the job’s transformations into <strong>stage</strong>. Each stage includes a colllection of <strong>tasks</strong> that run the same code on each subset of data.</p>\n<p>\\newpage</p>\n<h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM<a id=\"orgheadline8\"></a></h1><h2 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction<a id=\"orgheadline5\"></a></h2><p>Support Vector Machine is an effective and popular classification learning tool.</p>\n<p>Given a training set \\( S = { (x_i, y<em>i) }</em>{i=1}^{m} \\), where \\( x_i \\in \\mathbb{R}^n \\) and \\( y_i \\in { +1, -1 } \\), figure <a href=\"#orgparagraph3\">22</a> shows what a SVM looks like. We note  \\( w \\cdot x - b = 0 \\) as the hyperplane where \\( w \\) is the vector to the hyperplane. What we want is to find the maximum-margin hyperplane that divides the points having \\( y_i=1 \\) from those having \\( y_i=-1 \\). That means \\( y_i(w \\cdot x_i -b ) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\).</p>\n<p>So the optimization problem can be written as</p>\n<p>Maximize</p>\n<p>\\[ \\frac{2}{|w|} \\]</p>\n<p>Which equals to minimize</p>\n<p>\\[ \\frac{1}{2} | w |^2 \\]</p>\n<p>subject to \\( y_i(w \\cdot x_i - b) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\)</p>\n<p><img src=\"./imgs/svm.png\" alt=\"img\" title=\"SVM\"></p>\n<p>In fact, it can be viewed as an unconstrained empirical loss minimization with a penalty term for the norm of the classifier that is being learned. We would like to find the minimization of the problem</p>\n<p>\\[<br> \\min<em>{w} \\frac{\\lambda}{2}|w|^2 + \\frac{1}{m}\\sum</em>{(x, y) \\in S} \\ell(w; (x, y))<br>\\]</p>\n<p>Where &lambda; is the regularization parameter, ( \\ell(w, (x, y)) ) is the hinge loss:</p>\n<p>\\[<br>\\ell(w, (x, y)) = max{0, 1-y \\langle w, x \\rangle }<br>\\]</p>\n<p>To solve this optimization problem, we can use gradient descent to achieve the minimum value.</p>\n<p>The objective function is</p>\n<p>\\[<br>f(w) = \\frac{\\lambda}{2}|w|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))<br>\\]</p>\n<p>Then, the sub-gradient for iteration <em>t</em> is<br>\\[<br>\\nabla_t = \\lambda w<em>t - \\frac{1}{m}\\sum</em>{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i<br>\\]</p>\n<p>Now we can update \\( w \\), where \\( \\eta<em>t \\) is the step size<br>\\[<br>w</em>{t+1} \\leftarrow w_t - \\eta_t\\nabla_t<br>\\]</p>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD<a id=\"orgheadline6\"></a></h2><p>From the previous section, we notice that we need to iterate all the data point when calculating gradient.<br>And this might be computing expensive if we have tons of data. This is the reason why Stochastic Gradient Descent(SGD) becomes so useful.<br>When handling large scale problems, SGD uses sub dataset at each iteration instead of the whole dataset.</p>\n<p>So now, the object function becomes:<br>\\[<br>f(w, A<em>t) = \\frac{\\lambda}{2}|w|^2 + \\frac{1}{k}\\sum</em>{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))<br>\\]<br>where ( A_t \\subset S ), ( |A_t| = k ). At each iteration, we takes a subset of data point.</p>\n<p>And sub-gradient is<br> \\[ \\nabla_t = \\lambda w<em>t - \\frac{1}{k}\\sum</em>{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i \\]</p>\n<h2 id=\"Pegasos-and-MLlib-implementation\"><a href=\"#Pegasos-and-MLlib-implementation\" class=\"headerlink\" title=\"Pegasos and MLlib implementation\"></a>Pegasos and MLlib implementation<a id=\"orgheadline7\"></a></h2><p>Pegasos, is a kind of stochastic gradient descent algorithm. And Spark MLlib also provides an SGD implementation for us. After reading the code of MLLib, we notice that the only difference between Pegasos and MLlib is the choice of update step.</p>\n<p>\\[<br>w_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t<br>\\]</p>\n<p>In Pegasos, the update step is<br>\\[<br>\\eta_t = \\frac{\\alpha}{t\\lambda}<br>\\]</p>\n<p>In MLlib, this is<br>\\[<br>\\eta_t = \\frac{\\alpha}{\\sqrt{t}}<br>\\]</p>\n<p>where &alpha; is the step size parameter</p>\n<h1 id=\"SGD-in-Spark\"><a href=\"#SGD-in-Spark\" class=\"headerlink\" title=\"SGD in Spark\"></a>SGD in Spark<a id=\"orgheadline11\"></a></h1><h2 id=\"treeAggregate\"><a href=\"#treeAggregate\" class=\"headerlink\" title=\"treeAggregate\"></a>treeAggregate<a id=\"orgheadline9\"></a></h2><p>The main usage of Spark for SGD is to calculate the gradient which need to sum up the value of every data point. And in Spark, this is done by the RDD method <strong>treeAggregate</strong>. <strong>Aggregate</strong> is a generalized combination of <strong>Map</strong> and <strong>Reduce</strong>. The definition of <strong>treeAggregate</strong> is</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">RDD</span>.treeAggregate(zeroValue: <span class=\"type\">U</span>)(</span><br><span class=\"line\">      seqOp: (<span class=\"type\">U</span>, <span class=\"type\">T</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      combOp: (<span class=\"type\">U</span>, <span class=\"type\">U</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      depth: <span class=\"type\">Int</span> = <span class=\"number\">2</span>): <span class=\"type\">U</span></span><br></pre></td></tr></table></figure>\n<p>In this method, there are three parameters in which the first two are more important for us.</p>\n<ul>\n<li>seqOp: calculate sub gradient for every partition</li>\n<li>combOp: combine the result of seqOp or upper level combOp together</li>\n<li>depth: control the depth of the aggregation tree</li>\n</ul>\n<p><img src=\"./imgs/tree.png\" alt=\"img\" title=\"tree aggregate\"></p>\n<p>We can see from the figure <a href=\"#orgparagraph4\">46</a> that the first thing is to use the <strong>seqOp</strong> to calculate the sub gradient for every partition, then it sums them up level by level using <strong>combOp</strong>.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation<a id=\"orgheadline10\"></a></h2><p>In this section, the code for the main SGD logic is shown in <a href=\"#orgsrcblock1\">2</a>. It runs <code>numIterations</code> times to get the final ( w ).</p>\n<p>First, <code>data.sample</code> takes a subset of data whose size is decided by <code>miniBatchFraction</code>. Then we use <code>treeAggregate</code> method on this sample. In <code>seqOp</code>, The <code>gradientSum</code> is updated by <code>axpy(y, b_x, c._1)</code> if ( y\\langle w, x \\rangle &lt; 1 ) which means wrong classification. In <code>combOp</code>, <code>gradientSum</code> is combined together by <code>c1._1 += c2._1</code>. After we get the <code>gradientSum</code>, we calcuate <code>step</code> and <code>gradient</code>. Finally, we update the weights with <code>axpy(-step, gradient, weights)</code></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to numIterations) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> bcWeights = sc.broadcast(weights)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> (gradientSum, lossSum, batchSize) = data.sample(<span class=\"literal\">false</span>,</span><br><span class=\"line\">    miniBatchFraction, <span class=\"number\">42</span> + i)</span><br><span class=\"line\">        .treeAggregate((<span class=\"type\">BDV</span>.zeros[<span class=\"type\">Double</span>](weights.size), <span class=\"number\">0.0</span>, <span class=\"number\">0</span>L))(</span><br><span class=\"line\">          seqOp = (c, v) =&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">// c: (grad, loss, count), v: (label, features)</span></span><br><span class=\"line\">            <span class=\"keyword\">val</span> y = v.label</span><br><span class=\"line\">            <span class=\"keyword\">val</span> x = v.features</span><br><span class=\"line\">            <span class=\"keyword\">val</span> b_x = <span class=\"type\">BDV</span>(x.toArray)</span><br><span class=\"line\">            <span class=\"keyword\">val</span> dotProduct = bcWeights.value.dot(b_x)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (y * dotProduct &lt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">              axpy(y, b_x, c._1)    <span class=\"comment\">// add to gradientSum</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            (c._1, c._2 + math.max(<span class=\"number\">0</span>, <span class=\"number\">1</span> - y * dotProduct), c._3 + <span class=\"number\">1</span>)</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          combOp = (c1, c2) =&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">// c: (grad, loss, count)</span></span><br><span class=\"line\">            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class=\"line\">          &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> step = stepSize / (regParam * i)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> gradient = weights * regParam - gradientSum / batchSize.toDouble</span><br><span class=\"line\">      axpy(-step, gradient, weights)    <span class=\"comment\">// update weights</span></span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>\\newpage</p>\n<h1 id=\"Experiments-and-Performance\"><a href=\"#Experiments-and-Performance\" class=\"headerlink\" title=\"Experiments and Performance\"></a>Experiments and Performance<a id=\"orgheadline15\"></a></h1><h2 id=\"Experiments-with-small-dataset\"><a href=\"#Experiments-with-small-dataset\" class=\"headerlink\" title=\"Experiments with small dataset\"></a>Experiments with small dataset<a id=\"orgheadline12\"></a></h2><p>The first thing we need to do is to show that our Pegasos implementation works correctly. To achieve that, we simulate some sample 2D and 3D data with normal distribution.<br>The first one is a 2D linear dataset. The result is in figure <a href=\"#orgparagraph5\">52</a>.</p>\n<p><img src=\"./imgs/2d_linear.png\" alt=\"img\" title=\"2D linear\"></p>\n<p>Then this figure <a href=\"#orgparagraph6\">54</a> shows the result of a 3D linear dataset.</p>\n<p><img src=\"./imgs/3d_linear.png\" alt=\"img\" title=\"3D linear\"></p>\n<p>From those experiments, we show that our Pegasos implementation works well. Then we will test it’s performance with larger dataset.</p>\n<h2 id=\"The-convergence-speed\"><a href=\"#The-convergence-speed\" class=\"headerlink\" title=\"The convergence speed\"></a>The convergence speed<a id=\"orgheadline13\"></a></h2><p>Since the implementation of Pegasos and MLlib is slightly different. we would like to compare the convergence speed of Pegasos and MLlib. In this test, we take 5GB data with 1000 features, launch the job with 4 executors and run 100 iterations. The result is in figure <a href=\"#orgparagraph7\">57</a>, where the Y axis is not aligned. In the plot, the first 30 iterations are ignored since the initial loss is too high.</p>\n<p>&lt;./imgs/step1.eps&gt;</p>\n<p>Then we align the Y axis in figure <a href=\"#orgparagraph8\">59</a>. From those 2 figures, we can see that when the step size is well chosen, Pegasos and MLlib have similar performance. But Pegasos has one advantage that it is easier to find the right step parameter. In most cases, 1 is good for Pegasos.</p>\n<p>&lt;./imgs/step2.eps&gt;</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference<a id=\"orgheadline18\"></a></h1><ol>\n<li>Zaharia, Matei, et al. “Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.” Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012</li>\n<li>Zaharia, Matei, et al. “Spark: cluster computing with working sets.” Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010</li>\n<li>Shalev-Shwartz, Shai, et al. “Pegasos: Primal estimated sub-gradient solver for svm.” Mathematical programming 127.1 (2011): 3-30</li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}