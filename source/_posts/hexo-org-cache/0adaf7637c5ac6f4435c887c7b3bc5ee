{"md5":"df5a8e2084c3a97cb87e4a84df85f424","content":"\n<div id=\"outline-container-sec-1\" class=\"outline-2\">\n<h2 id=\"sec-1\"><span class=\"section-number-2\">1</span> SVM</h2>\n<div class=\"outline-text-2\" id=\"text-1\">\n</div><div id=\"outline-container-sec-1-1\" class=\"outline-3\">\n<h3 id=\"sec-1-1\"><span class=\"section-number-3\">1.1</span> &#x4ECB;&#x7ECD;</h3>\n<div class=\"outline-text-3\" id=\"text-1-1\">\n<p>\nSupport Vector Machine &#x652F;&#x6301;&#x5411;&#x91CF;&#x673A;&#x662F;&#x4E00;&#x79CD;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x3002; \n</p>\n\n<p>\n&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x8BAD;&#x7EC3;&#x96C6; \\( S = \\{ (x_i, y_i) \\}_{i=1}^{m} \\), &#x5176;&#x4E2D; \\( x_i \\in \\mathbb{R}^n \\) &#x5E76;&#x4E14; \\( y_i \\in \\{ +1, -1 \\} \\),\n&#x56FE;<a href=\"#svm\">1</a>&#x5C55;&#x793A;&#x4E86;&#x4E00;&#x4E2A; SVM &#x9700;&#x8981;&#x89E3;&#x51B3;&#x7684;&#x95EE;&#x9898;&#x3002; &#x6211;&#x4EEC;&#x6807;&#x8BB0;  \\( w \\cdot x - b = 0 \\) &#x4E3A;&#x8D85;&#x5E73;&#x9762;&#xFF0C; \\( w \\) &#x4EE3;&#x8868;&#x8BE5;&#x8D85;&#x5E73;&#x9762;&#x7684;&#x5411;&#x91CF;&#x3002; \n&#x6211;&#x4EEC;&#x9700;&#x8981;&#x505A;&#x7684;&#x662F;&#x627E;&#x5230;&#x80FD;&#x5C06; \\( y_i=1 \\) &#x7684;&#x70B9;&#x548C; \\( y_i=-1 \\) &#x7684;&#x70B9; &#x5206;&#x5F00;&#x7684;&#x8FB9;&#x9645;&#x6700;&#x5927;&#x7684;&#x8D85;&#x5E73;&#x9762;.\n&#x8FD9;&#x5C31;&#x610F;&#x5473;&#x7740; \\( y_i(w \\cdot x_i -b ) \\geq 1 \\)&#xFF0C;&#x5BF9;&#x4E8E;&#x6240;&#x6709; \\( 1 \\leq i \\leq n \\).\n</p>\n\n<p>\n&#x6240;&#x4EE5;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x53EF;&#x4EE5;&#x5199;&#x6210;&#xFF1A;\n</p>\n\n<p>\n&#x6700;&#x5927;&#x5316;\n</p>\n\n<p>\n\\[ \\frac{2}{\\|w\\|} \\]\n</p>\n\n<p>\n&#x8FD9;&#x7B49;&#x4EF7;&#x4E8E;&#x6700;&#x5C0F;&#x5316;\n</p>\n\n<p>\n\\[ \\frac{1}{2} \\| w \\|^2 \\]\n</p>\n\n<p>\nsubject to \\( y_i(w \\cdot x_i - b) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\)\n</p>\n\n\n<div id=\"svm\" class=\"figure\">\n<p><img src=\"images/svm/svm.png\" alt=\"captionm\" width=\"400px\">\n</p>\n<p><span class=\"figure-number\">Figure 1:</span> SVM</p>\n</div>\n\n<p>\n&#x4E8B;&#x5B9E;&#x4E0A;&#xFF0C;&#x5B83;&#x53EF;&#x4EE5;&#x88AB;&#x770B;&#x4F5C;&#x4E00;&#x4E2A;&#x5E26;&#x6709;&#x60E9;&#x7F5A;&#x9879;&#x7684;&#x6700;&#x5C0F;&#x5316;&#x635F;&#x5931;&#x95EE;&#x9898;&#x3002;&#x6700;&#x7EC8;&#xFF0C;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x627E;&#x5230;&#x4EE5;&#x4E0B;&#x95EE;&#x9898;&#x7684;&#x6700;&#x5C0F;&#x89E3;\n</p>\n\n<p>\n\\[\n \\min_{w} \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x, y) \\in S} \\ell(w; (x, y))\n\\]\n</p>\n\n<p>\n&#x5176;&#x4E2D; &#x3BB; &#x662F;&#x6B63;&#x89C4;&#x5316;&#x53C2;&#x6570;, \\( \\ell(w, (x, y)) \\) &#x662F; hinge &#x635F;&#x5931;&#x51FD;&#x6570;:\n</p>\n\n<p>\n\\[\n\\ell(w, (x, y)) = max\\{0, 1-y \\langle w, x \\rangle \\}\n\\]\n</p>\n\n<p>\n&#x5BF9;&#x4E8E;&#x8FD9;&#x4E00;&#x6700;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x6765;&#x8FBE;&#x5230;&#x6700;&#x5C0F;&#x503C;&#x3002;\n</p>\n\n<p>\n&#x76EE;&#x6807;&#x51FD;&#x6570;&#x4E3A;&#xFF1A;\n</p>\n\n<p>\n\\[\nf(w) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))\n\\]\n</p>\n\n<p>\n&#x6240;&#x4EE5;&#xFF0C;&#x8FED;&#x4EE3; <i>t</i> &#x65F6;&#x7684;&#x68AF;&#x5EA6;&#x4E3A;&#xFF1A;\n</p>\n\n<p>\n\\[\n\\nabla_t = \\lambda w_t - \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i\n\\]\n</p>\n\n<p>\n&#x4E8E;&#x662F;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x66F4;&#x65B0;  \\( w \\), &#x5176;&#x4E2D; \\( \\eta_t \\) &#x662F;&#x4E0B;&#x964D;&#x901F;&#x5EA6;\n\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\]\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-1-2\" class=\"outline-3\">\n<h3 id=\"sec-1-2\"><span class=\"section-number-3\">1.2</span> SGD</h3>\n<div class=\"outline-text-3\" id=\"text-1-2\">\n<p>\n&#x4ECE;&#x4E0A;&#x4E00;&#x8282;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x6211;&#x4EEC;&#x90FD;&#x9700;&#x8981;&#x6240;&#x6709;&#x7684;&#x6570;&#x636E;&#x70B9;&#x6765;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x3002;&#x800C;&#x5F53;&#x6570;&#x636E;&#x96C6;&#x53D8;&#x5927;&#x540E;&#xFF0C;&#x65E0;&#x7591;&#x4F1A;&#x8017;&#x8D39;&#x5927;&#x91CF;&#x7684;&#x8BA1;&#x7B97;&#x65F6;&#x95F4;&#x3002;\n&#x8FD9;&#x5C31;&#x662F;&#x4E3A;&#x4EC0;&#x4E48;&#x5728;&#x5927;&#x89C4;&#x6A21;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x4F1A;&#x4F7F;&#x7528; SGD&#xFF08;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#xFF09;&#x3002;SDG &#x5728;&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x65F6;&#x53EA;&#x4F7F;&#x7528;&#x4E00;&#x90E8;&#x5206;&#x6570;&#x636E;&#x800C;&#x4E0D;&#x662F;&#x5168;&#x90E8;&#xFF0C;\n&#x4ECE;&#x800C;&#x964D;&#x4F4E;&#x4E86;&#x8BA1;&#x7B97;&#x91CF;&#x3002;\n</p>\n\n<p>\n&#x6240;&#x4EE5;&#xFF0C;&#x73B0;&#x5728;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x53D8;&#x6210;&#x4E86;&#xFF1A;\n\\[\nf(w, A_t) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))\n\\]\nwhere \\( A_t \\subset S \\), \\( |A_t| = k \\). At each iteration, we takes a subset of data point.\n</p>\n\n<p>\n&#x7136;&#x540E;&#x68AF;&#x5EA6;&#x4E3A;&#xFF1A;\n \\[ \\nabla_t = \\lambda w_t - \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle &lt; 1]y_i x_i \\]\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-1-3\" class=\"outline-3\">\n<h3 id=\"sec-1-3\"><span class=\"section-number-3\">1.3</span> Pegasos and MLlib implementation</h3>\n<div class=\"outline-text-3\" id=\"text-1-3\">\n<p>\nPegasos &#x662F; SVM &#x4F7F;&#x7528;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x7684;&#x4E00;&#x79CD;&#x5B9E;&#x73B0;&#x3002;Spark MLlib &#x4E5F;&#x63D0;&#x4F9B;&#x4E86; SVM &#x7684;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x5B9E;&#x73B0;&#xFF0C;&#x4E8E; Pegasos &#x7A0D;&#x6709;&#x4E0D;&#x540C;&#x3002;\n&#x4E3B;&#x8981;&#x662F;&#x68AF;&#x5EA6;&#x7684;&#x66F4;&#x65B0;&#x901F;&#x5EA6;&#x4E0D;&#x540C;&#x3002;\n</p>\n\n<p>\n\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\]\n</p>\n\n<p>\n&#x5728; Pegasos &#x7B97;&#x6CD5;&#x4E2D;, &#x66F4;&#x65B0;&#x901F;&#x5EA6;&#x4E3A;\n\\[\n\\eta_t = \\frac{\\alpha}{t\\lambda}\n\\]\n</p>\n\n<p>\n&#x800C;&#x5728; MLlib &#x4E2D;&#xFF0C;&#x4E3A;&#xFF1A;\n\\[\n\\eta_t = \\frac{\\alpha}{\\sqrt{t}}\n\\]\n</p>\n\n<p>\n&#x5176;&#x4E2D; &#x3B1; &#x662F;&#x66F4;&#x65B0;&#x901F;&#x5EA6;&#x53C2;&#x6570;&#x3002;\n</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2\" class=\"outline-2\">\n<h2 id=\"sec-2\"><span class=\"section-number-2\">2</span> SGD in Spark</h2>\n<div class=\"outline-text-2\" id=\"text-2\">\n</div><div id=\"outline-container-sec-2-1\" class=\"outline-3\">\n<h3 id=\"sec-2-1\"><span class=\"section-number-3\">2.1</span> treeAggregate</h3>\n<div class=\"outline-text-3\" id=\"text-2-1\">\n<p>\nSpark &#x6765;&#x8BA1;&#x7B97; SGD &#x7684;&#x4E3B;&#x8981;&#x4F18;&#x52BF;&#x4F7F;&#x53EF;&#x4EE5;&#x5206;&#x5E03;&#x5F0F;&#x5730;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x5B83;&#x4EEC;&#x7D2F;&#x52A0;&#x8D77;&#x6765;&#x3002;\n&#x5728; Spark &#x4E2D;&#xFF0C;&#x8FD9;&#x4E00;&#x4EFB;&#x52A1;&#x662F;&#x901A;&#x8FC7; RDD &#x7684; <b>treeAggregate</b> &#x65B9;&#x6CD5;&#x6765;&#x5B8C;&#x6210;&#x7684;&#x3002;\n<b>Aggregate</b> &#x53EF;&#x88AB;&#x89C6;&#x4E3A;&#x6CDB;&#x5316;&#x7684; <b>Map</b> &#x548C; <b>Reduce</b> &#x7684;&#x7EC4;&#x5408;&#x3002; <b>treeAggregate</b> &#x7684;&#x5B9A;&#x4E49;&#x4E3A;\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">RDD</span>.treeAggregate(zeroValue: <span class=\"type\">U</span>)(</span><br><span class=\"line\">      seqOp: (<span class=\"type\">U</span>, <span class=\"type\">T</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      combOp: (<span class=\"type\">U</span>, <span class=\"type\">U</span>) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      depth: <span class=\"type\">Int</span> = <span class=\"number\">2</span>): <span class=\"type\">U</span></span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n&#x5728;&#x6B64;&#x65B9;&#x6CD5;&#x4E2D;&#x6709;&#x4E09;&#x4E2A;&#x53C2;&#x6570;&#xFF0C;&#x5176;&#x4E2D;&#x524D;&#x4E24;&#x4E2A;&#x5BF9;&#x6211;&#x4EEC;&#x66F4;&#x91CD;&#x8981;&#xFF1A;\n</p>\n\n<ul class=\"org-ul\">\n<li>seqOp: &#x8BA1;&#x7B97;&#x6BCF;&#x9694; partition &#x4E2D;&#x7684;&#x5B50;&#x68AF;&#x5EA6;\n</li>\n<li>combOp: &#x5C06; seqOp &#x6216;&#x4E0A;&#x5C42; combOp &#x7684;&#x503C;&#x5408;&#x5E76;\n</li>\n<li>depth: &#x63A7;&#x5236; tree &#x7684;&#x6DF1;&#x5EA6;\n</li>\n</ul>\n\n\n<div id=\"tree\" class=\"figure\">\n<p><img src=\"images/svm/tree.png\" alt=\"tree.png\">\n</p>\n<p><span class=\"figure-number\">Figure 2:</span> tree aggregate</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2-2\" class=\"outline-3\">\n<h3 id=\"sec-2-2\"><span class=\"section-number-3\">2.2</span> &#x5B9E;&#x73B0;</h3>\n<div class=\"outline-text-3\" id=\"text-2-2\">\n<p>\nSGD &#x662F;&#x4E00;&#x4E2A;&#x6C42;&#x6700;&#x4F18;&#x5316;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x8BB8;&#x591A;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x90FD;&#x53EF;&#x4EE5;&#x7528; SGD &#x6765;&#x6C42;&#x89E3;&#x3002;&#x6240;&#x4EE5; Spark &#x5BF9;&#x5176;&#x505A;&#x4E86;&#x62BD;&#x8C61;&#x3002;\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SVMWithSGD</span> <span class=\"title\">private</span> (</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> stepSize: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> numIterations: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> regParam: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> miniBatchFraction: <span class=\"type\">Double</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">GeneralizedLinearAlgorithm</span>[<span class=\"type\">SVMModel</span>] <span class=\"keyword\">with</span> <span class=\"type\">Serializable</span> {</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> gradient = <span class=\"keyword\">new</span> <span class=\"type\">HingeGradient</span>()</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> updater = <span class=\"keyword\">new</span> <span class=\"type\">SquaredL2Updater</span>()</span><br><span class=\"line\">  <span class=\"annotation\">@Since</span>(<span class=\"string\">&quot;0.8.0&quot;</span>)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> optimizer = <span class=\"keyword\">new</span> <span class=\"type\">GradientDescent</span>(gradient, updater)</span><br><span class=\"line\">    .setStepSize(stepSize)</span><br><span class=\"line\">    .setNumIterations(numIterations)</span><br><span class=\"line\">    .setRegParam(regParam)</span><br><span class=\"line\">    .setMiniBatchFraction(miniBatchFraction)</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n&#x53EF;&#x4EE5;&#x770B;&#x5230; <code>SVMWithSGD</code> &#x7EE7;&#x627F;&#x4E86; <code>GeneralizedLinearAlgorithm</code> &#xFF0C;&#x5E76;&#x5B9A;&#x4E49; <code>optimizer</code> &#x6765;&#x786E;&#x5B9A;&#x5982;&#x4F55;&#x83B7;&#x5F97;&#x4F18;&#x5316;&#x89E3;&#x3002;\n&#x800C; <code>optimizer</code> &#x5373;&#x662F; SGD &#x7B97;&#x6CD5;&#x7684;&#x5B9E;&#x73B0;&#x3002;&#x6B63;&#x5982;&#x4E0A;&#x8282;&#x6240;&#x8FF0;&#xFF0C;&#x7EBF;&#x6027; SVM &#x5B9E;&#x9645;&#x4E0A;&#x662F;&#x4F7F;&#x7528; hinge &#x635F;&#x5931;&#x51FD;&#x6570;&#x548C;&#x4E00;&#x4E2A; L2 &#x60E9;&#x7F5A;&#x9879;&#x7684;&#x7EBF;&#x6027;&#x6A21;&#x578B;&#xFF0C;&#x56E0;&#x6B64;&#x8FD9;&#x91CC;&#x4F7F;&#x7528;&#x4E86; <code>HingeGradient</code> &#x548C; <code>SquaredL2Updater</code> \n&#x4F5C;&#x4E3A; <code>GradientDescent</code> &#x7684;&#x53C2;&#x6570;&#x3002;\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HingeGradient</span> <span class=\"keyword\"><span class=\"keyword\">extends</span></span> <span class=\"title\">Gradient</span> {</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span>data: <span class=\"type\">Vector</span>, label: <span class=\"type\">Double</span>, weights: <span class=\"type\">Vector</span>): (<span class=\"type\">Vector</span>, <span class=\"type\">Double</span>) = {</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dotProduct = dot(data, weights)</span><br><span class=\"line\">    <span class=\"comment\">// Our loss function with {0, 1} labels is max(0, 1 - (2y - 1) (f_w(x)))</span></span><br><span class=\"line\">    <span class=\"comment\">// Therefore the gradient is -(2y - 1)*x</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> labelScaled = <span class=\"number\">2</span> * label - <span class=\"number\">1.0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">1.0</span> &gt; labelScaled * dotProduct) {</span><br><span class=\"line\">      <span class=\"keyword\">val</span> gradient = data.copy</span><br><span class=\"line\">      scal(-labelScaled, gradient)</span><br><span class=\"line\">      (gradient, <span class=\"number\">1.0</span> - labelScaled * dotProduct)</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">      (<span class=\"type\">Vectors</span>.sparse(weights.size, <span class=\"type\">Array</span>.empty, <span class=\"type\">Array</span>.empty), <span class=\"number\">0.0</span>)</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span></span><br><span class=\"line\">      data: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      label: <span class=\"type\">Double</span>,</span><br><span class=\"line\">      weights: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      cumGradient: <span class=\"type\">Vector</span>): <span class=\"type\">Double</span> = {</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dotProduct = dot(data, weights)</span><br><span class=\"line\">    <span class=\"comment\">// Our loss function with {0, 1} labels is max(0, 1 - (2y - 1) (f_w(x)))</span></span><br><span class=\"line\">    <span class=\"comment\">// Therefore the gradient is -(2y - 1)*x</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> labelScaled = <span class=\"number\">2</span> * label - <span class=\"number\">1.0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">1.0</span> &gt; labelScaled * dotProduct) {</span><br><span class=\"line\">      axpy(-labelScaled, data, cumGradient)</span><br><span class=\"line\">      <span class=\"number\">1.0</span> - labelScaled * dotProduct</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">      <span class=\"number\">0.0</span></span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span><br><span class=\"line\"> * :: DeveloperApi ::</span><br><span class=\"line\"> * Updater for L2 regularized problems.</span><br><span class=\"line\"> *          R(w) = 1/2 ||w||^2</span><br><span class=\"line\"> * Uses a step-size decreasing with the square root of the number of iterations.</span><br><span class=\"line\"> */</span></span><br><span class=\"line\"><span class=\"annotation\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SquaredL2Updater</span> <span class=\"keyword\"><span class=\"keyword\">extends</span></span> <span class=\"title\">Updater</span> {</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span></span><br><span class=\"line\">      weightsOld: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      gradient: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      stepSize: <span class=\"type\">Double</span>,</span><br><span class=\"line\">      iter: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      regParam: <span class=\"type\">Double</span>): (<span class=\"type\">Vector</span>, <span class=\"type\">Double</span>) = {</span><br><span class=\"line\">    <span class=\"comment\">// add up both updates from the gradient of the loss (= step) as well as</span></span><br><span class=\"line\">    <span class=\"comment\">// the gradient of the regularizer (= regParam * weightsOld)</span></span><br><span class=\"line\">    <span class=\"comment\">// w&apos; = w - thisIterStepSize * (gradient + regParam * w)</span></span><br><span class=\"line\">    <span class=\"comment\">// w&apos; = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> thisIterStepSize = stepSize / math.sqrt(iter)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> brzWeights: <span class=\"type\">BV</span>[<span class=\"type\">Double</span>] = weightsOld.asBreeze.toDenseVector</span><br><span class=\"line\">    brzWeights :*= (<span class=\"number\">1.0</span> - thisIterStepSize * regParam)</span><br><span class=\"line\">    brzAxpy(-thisIterStepSize, gradient.asBreeze, brzWeights)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> norm = brzNorm(brzWeights, <span class=\"number\">2.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    (<span class=\"type\">Vectors</span>.fromBreeze(brzWeights), <span class=\"number\">0.5</span> * regParam * norm * norm)</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n&#x6B64;&#x8282;&#x4E2D;, <a href=\"#code\">1</a> &#x5C55;&#x793A;&#x4E86; <code>GradientDescent</code> &#x7684;&#x4E3B;&#x8981;&#x6267;&#x884C;&#x903B;&#x8F91;&#x3002; &#x91CD;&#x590D;&#x6267;&#x884C; <code>numIterations</code> &#x6B21;&#x4EE5;&#x83B7;&#x5F97;&#x6700;&#x7EC8;&#x7684; \\( w \\)&#x3002;\n</p>\n\n<p>\n&#x9996;&#x5148;, <code>data.sample</code> &#x901A;&#x8FC7; <code>miniBatchFraction</code> &#x53D6;&#x4E00;&#x90E8;&#x5206;&#x6837;&#x672C;. &#x7136;&#x540E;&#x4F7F;&#x7528; <code>treeAggregate</code> &#x3002;\n&#x5728; <code>seqOp</code> &#x4E2D;, <code>gradientSum</code> &#x4F1A;&#x901A;&#x8FC7; <code>axpy(y, b_x, c._1)</code> &#x66F4;&#x65B0;&#xFF0C;&#x5982;&#x679C; \\( y\\langle w, x \\rangle &lt; 1 \\)&#xFF0C;&#x5373;&#x5206;&#x7C7B;&#x9519;&#x8BEF;&#x3002;\n&#x5728; <code>combOp</code> &#x4E2D;, <code>gradientSum</code> &#x901A;&#x8FC7; <code>c1._1 += c2._1</code> &#x88AB;&#x96C6;&#x5408;&#x8D77;&#x6765;&#x3002; &#x5F53;&#x83B7;&#x5F97; <code>gradientSum</code> &#x540E;, &#x6211;&#x4EEC;&#x5C31;&#x53EF;&#x4EE5;&#x8BA1;&#x7B97; <code>step</code> &#x548C; <code>gradient</code> &#x4E86;&#x3002;\n&#x6700;&#x540E;, &#x6211;&#x4EEC;&#x4F7F;&#x7528; <code>axpy(-step, gradient, weights)</code> &#x66F4;&#x65B0; <code>weights</code> &#x3002;\n</p>\n\n<div class=\"org-src-container\">\n<label class=\"org-src-name\">GradientDescent &#x4EE3;&#x7801;&#x7247;&#x65AD;</label>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (!converged &amp;&amp; i &lt;= numIterations) {</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bcWeights = data.context.broadcast(weights)</span><br><span class=\"line\">  <span class=\"comment\">// Sample a subset (fraction miniBatchFraction) of the total data</span></span><br><span class=\"line\">  <span class=\"comment\">// compute and sum up the subgradients on this subset (this is one map-reduce)</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> (gradientSum, lossSum, miniBatchSize) = data.sample(<span class=\"literal\">false</span>, miniBatchFraction, <span class=\"number\">42</span> + i)</span><br><span class=\"line\">    .treeAggregate((<span class=\"type\">BDV</span>.zeros[<span class=\"type\">Double</span>](n), <span class=\"number\">0.0</span>, <span class=\"number\">0</span>L))(</span><br><span class=\"line\">      seqOp = (c, v) =&gt; {</span><br><span class=\"line\">\t<span class=\"comment\">// c: (grad, loss, count), v: (label, features)</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> l = gradient.compute(v._2, v._1, bcWeights.value, <span class=\"type\">Vectors</span>.fromBreeze(c._1))</span><br><span class=\"line\">\t(c._1, c._2 + l, c._3 + <span class=\"number\">1</span>)</span><br><span class=\"line\">      },</span><br><span class=\"line\">      combOp = (c1, c2) =&gt; {</span><br><span class=\"line\">\t<span class=\"comment\">// c: (grad, loss, count)</span></span><br><span class=\"line\">\t(c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class=\"line\">      })</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (miniBatchSize &gt; <span class=\"number\">0</span>) {</span><br><span class=\"line\">    <span class=\"comment\">/**</span><br><span class=\"line\">     * lossSum is computed using the weights from the previous iteration</span><br><span class=\"line\">     * and regVal is the regularization value computed in the previous iteration as well.</span><br><span class=\"line\">     */</span></span><br><span class=\"line\">    stochasticLossHistory.append(lossSum / miniBatchSize + regVal)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> update = updater.compute(</span><br><span class=\"line\">      weights, <span class=\"type\">Vectors</span>.fromBreeze(gradientSum / miniBatchSize.toDouble),</span><br><span class=\"line\">      stepSize, i, regParam)</span><br><span class=\"line\">    weights = update._1</span><br><span class=\"line\">    regVal = update._2</span><br><span class=\"line\"></span><br><span class=\"line\">    previousWeights = currentWeights</span><br><span class=\"line\">    currentWeights = <span class=\"type\">Some</span>(weights)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (previousWeights != <span class=\"type\">None</span> &amp;&amp; currentWeights != <span class=\"type\">None</span>) {</span><br><span class=\"line\">      converged = isConverged(previousWeights.get,</span><br><span class=\"line\">\tcurrentWeights.get, convergenceTol)</span><br><span class=\"line\">    }</span><br><span class=\"line\">  } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">    logWarning(s<span class=\"string\">&quot;Iteration ($i/$numIterations). The size of sampled batch is zero&quot;</span>)</span><br><span class=\"line\">  }</span><br><span class=\"line\">  i += <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n</div>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-3\" class=\"outline-2\">\n<h2 id=\"sec-3\"><span class=\"section-number-2\">3</span> &#x5B9E;&#x9A8C;&#x548C;&#x6027;&#x80FD;</h2>\n<div class=\"outline-text-2\" id=\"text-3\">\n</div><div id=\"outline-container-sec-3-1\" class=\"outline-3\">\n<h3 id=\"sec-3-1\"><span class=\"section-number-3\">3.1</span> &#x6B63;&#x786E;&#x6027;&#x9A8C;&#x8BC1;</h3>\n<div class=\"outline-text-3\" id=\"text-3-1\">\n<p>\n&#x6211;&#x4EEC;&#x6A21;&#x62DF;&#x4E86;&#x4E00;&#x4E9B;&#x7B80;&#x5355;&#x7684; 2D &#x548C; 3D &#x6570;&#x636E;&#x6765;&#x9A8C;&#x8BC1;&#x6B63;&#x786E;&#x6027;&#x3002;\n</p>\n\n<div id=\"2d-linear\" class=\"figure\">\n<p><img src=\"images/svm/2d_linear.png\" alt=\"2d_linear.png\">\n</p>\n<p><span class=\"figure-number\">Figure 3:</span> 2D linear</p>\n</div>\n\n\n<div id=\"3d-linear\" class=\"figure\">\n<p><img src=\"images/svm/3d_linear.png\" alt=\"3d_linear.png\">\n</p>\n<p><span class=\"figure-number\">Figure 4:</span> 3D linear</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-3-2\" class=\"outline-3\">\n<h3 id=\"sec-3-2\"><span class=\"section-number-3\">3.2</span> &#x6536;&#x655B;&#x901F;&#x5EA6;</h3>\n<div class=\"outline-text-3\" id=\"text-3-2\">\n<p>\n&#x6211;&#x4EEC;&#x6BD4;&#x8F83;&#x4E24;&#x79CD;&#x5B9E;&#x73B0;&#x7684;&#x6536;&#x655B;&#x901F;&#x5EA6;&#x5DEE;&#x5F02;&#x3002;&#x8FD9;&#x91CC;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528; 5GB &#x5E26;&#x6709; 1000 &#x4E2A;&#x7279;&#x5F81;&#x7684;&#x6A21;&#x62DF;&#x6570;&#x636E;&#x3002;&#x4F7F;&#x7528; 4 &#x4E2A; executors &#x5E76;&#x8FED;&#x4EE3; 100 &#x6B21;&#x3002;\n</p>\n\n\n<div id=\"convergence1\" class=\"figure\">\n<p><img src=\"images/svm/step1.png\" alt=\"step1.png\">\n</p>\n<p><span class=\"figure-number\">Figure 5:</span> before aligning Y axis</p>\n</div>\n\n\n<div id=\"convergence2\" class=\"figure\">\n<p><img src=\"images/svm/step2.png\" alt=\"step2.png\">\n</p>\n<p><span class=\"figure-number\">Figure 6:</span> after aligning Y axis</p>\n</div>\n</div>\n</div>\n</div>\n\n\n<div id=\"outline-container-sec-4\" class=\"outline-2\">\n<h2 id=\"sec-4\"><span class=\"section-number-2\">4</span> &#x53C2;&#x8003;&#x6587;&#x732E;</h2>\n<div class=\"outline-text-2\" id=\"text-4\">\n<ol class=\"org-ol\">\n<li>Zaharia, Matei, et al. &quot;Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.&quot; Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012\n</li>\n<li>Zaharia, Matei, et al. &quot;Spark: cluster computing with working sets.&quot; Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010\n</li>\n<li>Shalev-Shwartz, Shai, et al. &quot;Pegasos: Primal estimated sub-gradient solver for svm.&quot; Mathematical programming 127.1 (2011): 3-30\n</li>\n</ol>\n</div>\n</div>\n"}
